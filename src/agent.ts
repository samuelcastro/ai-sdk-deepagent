/**
 * Deep Agent implementation using Vercel AI SDK v6 ToolLoopAgent.
 */

import {
  ToolLoopAgent,
  stepCountIs,
  generateText,
  streamText,
  wrapLanguageModel,
  type ToolSet,
  type StopCondition,
  type LanguageModel,
  type LanguageModelMiddleware,
} from "ai";
import type {
  CreateDeepAgentParams,
  DeepAgentState,
  BackendProtocol,
  BackendFactory,
  DeepAgentEvent,
  EventCallback,
  StreamWithEventsOptions,
  ModelMessage,
  SandboxBackendProtocol,
  InterruptOnConfig,
} from "./types.ts";
import type { BaseCheckpointSaver, Checkpoint, InterruptData } from "./checkpointer/types.ts";
import { isSandboxBackend } from "./types.ts";
import {
  BASE_PROMPT,
  TODO_SYSTEM_PROMPT,
  FILESYSTEM_SYSTEM_PROMPT,
  TASK_SYSTEM_PROMPT,
  EXECUTE_SYSTEM_PROMPT,
  buildSkillsPrompt,
} from "./prompts.ts";
import { createTodosTool } from "./tools/todos.ts";
import { createFilesystemTools } from "./tools/filesystem.ts";
import { createSubagentTool } from "./tools/subagent.ts";
import { createExecuteTool } from "./tools/execute.ts";
import { StateBackend } from "./backends/state.ts";
import { patchToolCalls } from "./utils/patch-tool-calls.ts";
import { summarizeIfNeeded } from "./utils/summarization.ts";
import { applyInterruptConfig, wrapToolsWithApproval, type ApprovalCallback } from "./utils/approval.ts";
import type { SummarizationConfig } from "./types.ts";

/**
 * Build the full system prompt from components.
 */
function buildSystemPrompt(
  customPrompt?: string,
  hasSubagents?: boolean,
  hasSandbox?: boolean,
  skills?: Array<{ name: string; description: string; path: string }>
): string {
  const parts = [
    customPrompt || "",
    BASE_PROMPT,
    TODO_SYSTEM_PROMPT,
    FILESYSTEM_SYSTEM_PROMPT,
  ];

  if (hasSandbox) {
    parts.push(EXECUTE_SYSTEM_PROMPT);
  }

  if (hasSubagents) {
    parts.push(TASK_SYSTEM_PROMPT);
  }

  // Add skills prompt if skills loaded
  if (skills && skills.length > 0) {
    parts.push(buildSkillsPrompt(skills));
  }

  return parts.filter(Boolean).join("\n\n");
}

/**
 * Deep Agent wrapper class that provides generate() and stream() methods.
 * Uses ToolLoopAgent from AI SDK v6 for the agent loop.
 */
export class DeepAgent {
  private model: LanguageModel;
  private systemPrompt: string;
  private userTools: ToolSet;
  private maxSteps: number;
  private backend: BackendProtocol | BackendFactory;
  private subagentOptions: {
    defaultModel: LanguageModel;
    defaultTools: ToolSet;
    subagents: CreateDeepAgentParams["subagents"];
    includeGeneralPurposeAgent: boolean;
  };
  private toolResultEvictionLimit?: number;
  private enablePromptCaching: boolean;
  private summarizationConfig?: SummarizationConfig;
  private hasSandboxBackend: boolean;
  private interruptOn?: InterruptOnConfig;
  private checkpointer?: BaseCheckpointSaver;
  private skillsMetadata: Array<{ name: string; description: string; path: string }> = [];

  constructor(params: CreateDeepAgentParams) {
    const {
      model,
      middleware,
      tools = {},
      systemPrompt,
      subagents = [],
      backend,
      maxSteps = 100,
      includeGeneralPurposeAgent = true,
      toolResultEvictionLimit,
      enablePromptCaching = false,
      summarization,
      interruptOn,
      checkpointer,
      skillsDir,
    } = params;

    // Wrap model with middleware if provided
    if (middleware) {
      const middlewares = Array.isArray(middleware)
        ? middleware
        : [middleware];

      this.model = wrapLanguageModel({
        model: model as any, // Cast required since wrapLanguageModel expects LanguageModelV3
        middleware: middlewares,
      }) as LanguageModel;
    } else {
      this.model = model;
    }
    this.maxSteps = maxSteps;
    this.backend =
      backend || ((state: DeepAgentState) => new StateBackend(state));
    this.toolResultEvictionLimit = toolResultEvictionLimit;
    this.enablePromptCaching = enablePromptCaching;
    this.summarizationConfig = summarization;
    this.interruptOn = interruptOn;
    this.checkpointer = checkpointer;

    // Load skills if directory provided
    if (skillsDir) {
      this.loadSkills(skillsDir).catch(error => {
        console.warn('[DeepAgent] Failed to load skills:', error);
      });
    }

    // Check if backend is a sandbox (supports execute)
    // For factory functions, we can't know until runtime, so we check if it's an instance
    this.hasSandboxBackend = typeof backend !== "function" && backend !== undefined && isSandboxBackend(backend);

    // Determine if we have subagents
    const hasSubagents =
      includeGeneralPurposeAgent || (subagents && subagents.length > 0);

    this.systemPrompt = buildSystemPrompt(systemPrompt, hasSubagents, this.hasSandboxBackend, this.skillsMetadata);

    // Store user-provided tools
    this.userTools = tools;

    // Store subagent options for later use
    this.subagentOptions = {
      defaultModel: model,
      defaultTools: tools,
      subagents,
      includeGeneralPurposeAgent,
    };
  }

  /**
   * Create all tools for a given state.
   * @param state - The shared agent state
   * @param onEvent - Optional callback for emitting events
   */
  private createTools(state: DeepAgentState, onEvent?: EventCallback): ToolSet {
    const todosTool = createTodosTool(state, onEvent);
    const filesystemTools = createFilesystemTools(state, {
      backend: this.backend,
      onEvent,
      toolResultEvictionLimit: this.toolResultEvictionLimit,
    });

    let allTools: ToolSet = {
      write_todos: todosTool,
      ...filesystemTools,
      ...this.userTools,
    };

    // Add execute tool if backend is a sandbox
    if (this.hasSandboxBackend) {
      const sandboxBackend = this.backend as SandboxBackendProtocol;
      allTools.execute = createExecuteTool({
        backend: sandboxBackend,
        onEvent,
      });
    }

    // Add subagent tool if configured
    if (
      this.subagentOptions.includeGeneralPurposeAgent ||
      (this.subagentOptions.subagents &&
        this.subagentOptions.subagents.length > 0)
    ) {
      const subagentTool = createSubagentTool(state, {
        defaultModel: this.subagentOptions.defaultModel,
        defaultTools: this.userTools,
        subagents: this.subagentOptions.subagents,
        includeGeneralPurposeAgent:
          this.subagentOptions.includeGeneralPurposeAgent,
        backend: this.backend,
        onEvent,
        interruptOn: this.interruptOn,
      });
      allTools.task = subagentTool;
    }

    // Apply interruptOn configuration to tools
    allTools = applyInterruptConfig(allTools, this.interruptOn);

    return allTools;
  }

  /**
   * Create a ToolLoopAgent for a given state.
   * @param state - The shared agent state
   * @param maxSteps - Optional max steps override
   * @param onEvent - Optional callback for emitting events
   */
  private createAgent(state: DeepAgentState, maxSteps?: number, onEvent?: EventCallback) {
    const tools = this.createTools(state, onEvent);

    return new ToolLoopAgent({
      model: this.model,
      instructions: this.systemPrompt,
      tools,
      stopWhen: stepCountIs(maxSteps ?? this.maxSteps),
    });
  }

  /**
   * Load skills from directory asynchronously.
   */
  private async loadSkills(skillsDir: string) {
    const { listSkills } = await import("./skills/load.ts");

    const skills = await listSkills({
      projectSkillsDir: skillsDir,
    });

    this.skillsMetadata = skills.map(s => ({
      name: s.name,
      description: s.description,
      path: s.path,
    }));
  }

  /**
   * Generate a response (non-streaming).
   */
  async generate(options: { prompt: string; maxSteps?: number }) {
    // Create fresh state for this invocation
    const state: DeepAgentState = {
      todos: [],
      files: {},
    };

    const agent = this.createAgent(state, options.maxSteps);
    const result = await agent.generate({ prompt: options.prompt });

    // Return result with state attached
    return {
      ...result,
      state,
    };
  }

  /**
   * Stream a response.
   */
  async stream(options: { prompt: string; maxSteps?: number }) {
    // Create fresh state for this invocation
    const state: DeepAgentState = {
      todos: [],
      files: {},
    };

    const agent = this.createAgent(state, options.maxSteps);
    const result = await agent.stream({ prompt: options.prompt });

    // Return result with state attached
    return {
      ...result,
      state,
    };
  }

  /**
   * Generate with an existing state (for continuing conversations).
   */
  async generateWithState(options: {
    prompt: string;
    state: DeepAgentState;
    maxSteps?: number;
  }) {
    const agent = this.createAgent(options.state, options.maxSteps);
    const result = await agent.generate({ prompt: options.prompt });

    return {
      ...result,
      state: options.state,
    };
  }

  /**
   * Get the underlying ToolLoopAgent for advanced usage.
   * This allows using AI SDK's createAgentUIStream and other utilities.
   */
  getAgent(state?: DeepAgentState) {
    const agentState = state || { todos: [], files: {} };
    return this.createAgent(agentState);
  }

  /**
   * Stream a response with real-time events.
   * This is an async generator that yields DeepAgentEvent objects.
   * 
   * Supports conversation history via the `messages` option for multi-turn conversations.
   * 
   * @example
   * ```typescript
   * // Single turn
   * for await (const event of agent.streamWithEvents({ prompt: "..." })) {
   *   switch (event.type) {
   *     case 'text':
   *       process.stdout.write(event.text);
   *       break;
   *     case 'done':
   *       // event.messages contains the updated conversation history
   *       console.log('Messages:', event.messages);
   *       break;
   *   }
   * }
   * 
   * // Multi-turn conversation
   * let messages = [];
   * for await (const event of agent.streamWithEvents({ prompt: "Hello", messages })) {
   *   if (event.type === 'done') {
   *     messages = event.messages; // Save for next turn
   *   }
   * }
   * for await (const event of agent.streamWithEvents({ prompt: "Follow up", messages })) {
   *   // Agent now has context from previous turn
   * }
   * ```
   */
  async *streamWithEvents(
    options: StreamWithEventsOptions
  ): AsyncGenerator<DeepAgentEvent, void, unknown> {
    const { threadId, resume } = options;
    
    // Load checkpoint if threadId is provided and checkpointer exists
    let state: DeepAgentState = options.state || { todos: [], files: {} };
    let patchedHistory: ModelMessage[] = [];
    let currentStep = 0;
    let pendingInterrupt: InterruptData | undefined;
    
    if (threadId && this.checkpointer) {
      const checkpoint = await this.checkpointer.load(threadId);
      if (checkpoint) {
        // Restore from checkpoint
        state = checkpoint.state;
        patchedHistory = checkpoint.messages;
        currentStep = checkpoint.step;
        pendingInterrupt = checkpoint.interrupt;
        
        yield {
          type: "checkpoint-loaded",
          threadId,
          step: checkpoint.step,
          messagesCount: checkpoint.messages.length,
        };
      }
    }
    
    // Handle resume from interrupt
    if (resume && pendingInterrupt) {
      // Process the resume decision (approve/deny the pending tool call)
      const decision = resume.decisions[0];
      if (decision?.type === 'approve') {
        // Clear the interrupt and continue
        pendingInterrupt = undefined;
      } else {
        // Deny - the tool was rejected, clear interrupt
        pendingInterrupt = undefined;
        // Could add a denied message to history here if needed
      }
    }
    
    // Require prompt unless resuming
    if (!options.prompt && !resume) {
      yield {
        type: "error",
        error: new Error("Either 'prompt' or 'resume' is required"),
      };
      return;
    }
    
    // If no prompt but resuming, use an empty prompt (the checkpoint has context)
    const prompt = options.prompt || "";

    // Build messages array: previous history + new user message
    // Patch any dangling tool calls in the history first
    patchedHistory = patchToolCalls(patchedHistory);

    // Apply summarization if enabled and needed
    if (this.summarizationConfig?.enabled && patchedHistory.length > 0) {
      const summarizationResult = await summarizeIfNeeded(patchedHistory, {
        model: this.summarizationConfig.model || this.model,
        tokenThreshold: this.summarizationConfig.tokenThreshold,
        keepMessages: this.summarizationConfig.keepMessages,
      });
      patchedHistory = summarizationResult.messages;
    }

    const inputMessages: ModelMessage[] = [
      ...patchedHistory,
      ...(prompt ? [{ role: "user", content: prompt } as ModelMessage] : []),
    ];

    // Event queue for collecting events from tool executions
    const eventQueue: DeepAgentEvent[] = [];
    let stepNumber = 0; // Relative to current execution
    const baseStep = currentStep; // Cumulative step from checkpoint

    // Event callback that tools will use to emit events
    const onEvent: EventCallback = (event) => {
      eventQueue.push(event);
    };

    // Create tools with event callback
    let tools = this.createTools(state, onEvent);

    // Wrap tools with approval checking if interruptOn is configured and callback provided
    // This intercepts tool execution and requests approval before running
    const hasInterruptOn = !!this.interruptOn;
    const hasApprovalCallback = !!options.onApprovalRequest;
    
    if (hasInterruptOn && hasApprovalCallback) {
      tools = wrapToolsWithApproval(tools, this.interruptOn, options.onApprovalRequest);
    }

    try {
      // Build streamText options
      const streamOptions: Parameters<typeof streamText>[0] = {
        model: this.model,
        messages: inputMessages,
        tools,
        stopWhen: stepCountIs(options.maxSteps ?? this.maxSteps),
        abortSignal: options.abortSignal,
        onStepFinish: async ({ toolCalls, toolResults }) => {
          stepNumber++;
          const cumulativeStep = baseStep + stepNumber;

          // Emit step finish event (relative step number)
          const stepEvent: DeepAgentEvent = {
            type: "step-finish",
            stepNumber,
            toolCalls: toolCalls.map((tc, i) => ({
              toolName: tc.toolName,
              args: "input" in tc ? tc.input : undefined,
              result: toolResults[i] ? ("output" in toolResults[i] ? toolResults[i].output : undefined) : undefined,
            })),
          };
          eventQueue.push(stepEvent);
          
          // Save checkpoint if configured
          if (threadId && this.checkpointer) {
            // Get current messages state - we need to track messages as they're built
            // For now, we'll save with the input messages (will be updated after assistant response)
            const checkpoint: Checkpoint = {
              threadId,
              step: cumulativeStep, // Cumulative step number
              messages: inputMessages, // Current messages before assistant response
              state: { ...state },
              interrupt: pendingInterrupt,
              createdAt: new Date().toISOString(),
              updatedAt: new Date().toISOString(),
            };
            await this.checkpointer.save(checkpoint);
            
            eventQueue.push({
              type: "checkpoint-saved",
              threadId,
              step: cumulativeStep,
            });
          }
        },
      };

      // Add system prompt with optional caching for Anthropic models
      if (this.enablePromptCaching) {
        // Use messages format with cache control for Anthropic
        streamOptions.messages = [
          {
            role: "system",
            content: this.systemPrompt,
            providerOptions: {
              anthropic: { cacheControl: { type: "ephemeral" } },
            },
          } as ModelMessage,
          ...inputMessages,
        ];
      } else {
        // Use standard system prompt
        streamOptions.system = this.systemPrompt;
      }

      // Use streamText with messages array for conversation history
      const result = streamText(streamOptions);

      // Yield step start event
      yield { type: "step-start", stepNumber: 1 };

      // Stream text chunks
      for await (const chunk of result.textStream) {
        // First, yield any queued events from tool executions
        while (eventQueue.length > 0) {
          const event = eventQueue.shift()!;
          yield event;
          
          // If a step finished, yield the next step start
          if (event.type === "step-finish") {
            yield { type: "step-start", stepNumber: event.stepNumber + 1 };
          }
        }

        // Then yield the text chunk
        if (chunk) {
          yield { type: "text", text: chunk };
        }
      }

      // Yield any remaining queued events
      while (eventQueue.length > 0) {
        yield eventQueue.shift()!;
      }

      // Get the final text
      const finalText = await result.text;

      // Build updated messages array with assistant response
      const updatedMessages: ModelMessage[] = [
        ...inputMessages,
        { role: "assistant", content: finalText } as ModelMessage,
      ];

      // Yield done event with updated messages
      yield {
        type: "done",
        state,
        text: finalText,
        messages: updatedMessages,
      };
      
      // Save final checkpoint after done event
      if (threadId && this.checkpointer) {
        const finalCheckpoint: Checkpoint = {
          threadId,
          step: baseStep + stepNumber, // Cumulative step number
          messages: updatedMessages,
          state,
          createdAt: new Date().toISOString(),
          updatedAt: new Date().toISOString(),
        };
        await this.checkpointer.save(finalCheckpoint);
      }
    } catch (error) {
      // Yield error event
      yield {
        type: "error",
        error: error instanceof Error ? error : new Error(String(error)),
      };
    }
  }

  /**
   * Stream with a simple callback interface.
   * This is a convenience wrapper around streamWithEvents.
   */
  async streamWithCallback(
    options: StreamWithEventsOptions,
    onEvent: EventCallback
  ): Promise<{ state: DeepAgentState; text?: string; messages?: ModelMessage[] }> {
    let finalState: DeepAgentState = options.state || { todos: [], files: {} };
    let finalText: string | undefined;
    let finalMessages: ModelMessage[] | undefined;

    for await (const event of this.streamWithEvents(options)) {
      onEvent(event);

      if (event.type === "done") {
        finalState = event.state;
        finalText = event.text;
        finalMessages = event.messages;
      }
    }

    return { state: finalState, text: finalText, messages: finalMessages };
  }
}

/**
 * Create a Deep Agent with planning, filesystem, and subagent capabilities.
 *
 * @param params - Configuration object for the Deep Agent
 * @param params.model - **Required.** AI SDK LanguageModel instance (e.g., `anthropic('claude-sonnet-4-20250514')`, `openai('gpt-4o')`)
 * @param params.systemPrompt - Optional custom system prompt for the agent
 * @param params.tools - Optional custom tools to add to the agent (AI SDK ToolSet)
 * @param params.subagents - Optional array of specialized subagent configurations for task delegation
 * @param params.backend - Optional backend for filesystem operations (default: StateBackend for in-memory storage)
 * @param params.maxSteps - Optional maximum number of steps for the agent loop (default: 100)
 * @param params.includeGeneralPurposeAgent - Optional flag to include general-purpose subagent (default: true)
 * @param params.toolResultEvictionLimit - Optional token limit before evicting large tool results to filesystem (default: disabled)
 * @param params.enablePromptCaching - Optional flag to enable prompt caching for improved performance (Anthropic only, default: false)
 * @param params.summarization - Optional summarization configuration for automatic conversation summarization
 * @returns A configured DeepAgent instance
 *
 * @see {@link CreateDeepAgentParams} for detailed parameter types
 *
 * @example Basic usage
 * ```typescript
 * import { createDeepAgent } from 'ai-sdk-deep-agent';
 * import { anthropic } from '@ai-sdk/anthropic';
 *
 * const agent = createDeepAgent({
 *   model: anthropic('claude-sonnet-4-20250514'),
 *   systemPrompt: 'You are a research assistant...',
 * });
 *
 * const result = await agent.generate({
 *   prompt: 'Research the topic and write a report',
 * });
 * ```
 *
 * @example With custom tools
 * ```typescript
 * import { tool } from 'ai';
 * import { z } from 'zod';
 *
 * const customTool = tool({
 *   description: 'Get current time',
 *   inputSchema: z.object({}),
 *   execute: async () => new Date().toISOString(),
 * });
 *
 * const agent = createDeepAgent({
 *   model: anthropic('claude-sonnet-4-20250514'),
 *   tools: { get_time: customTool },
 * });
 * ```
 *
 * @example With subagents
 * ```typescript
 * const agent = createDeepAgent({
 *   model: anthropic('claude-sonnet-4-20250514'),
 *   subagents: [{
 *     name: 'research-agent',
 *     description: 'Specialized for research tasks',
 *     systemPrompt: 'You are a research specialist...',
 *   }],
 * });
 * ```
 *
 * @example With StateBackend (default, explicit)
 * ```typescript
 * import { StateBackend } from 'ai-sdk-deep-agent';
 *
 * const state = { todos: [], files: {} };
 * const agent = createDeepAgent({
 *   model: anthropic('claude-sonnet-4-20250514'),
 *   backend: new StateBackend(state), // Ephemeral in-memory storage
 * });
 * ```
 *
 * @example With FilesystemBackend
 * ```typescript
 * import { FilesystemBackend } from 'ai-sdk-deep-agent';
 *
 * const agent = createDeepAgent({
 *   model: anthropic('claude-sonnet-4-20250514'),
 *   backend: new FilesystemBackend({ rootDir: './workspace' }), // Persist to disk
 * });
 * ```
 *
 * @example With PersistentBackend
 * ```typescript
 * import { PersistentBackend, InMemoryStore } from 'ai-sdk-deep-agent';
 *
 * const store = new InMemoryStore();
 * const agent = createDeepAgent({
 *   model: anthropic('claude-sonnet-4-20250514'),
 *   backend: new PersistentBackend({ store, namespace: 'project-1' }), // Cross-session persistence
 * });
 * ```
 *
 * @example With CompositeBackend
 * ```typescript
 * import { CompositeBackend, FilesystemBackend, StateBackend } from 'ai-sdk-deep-agent';
 *
 * const state = { todos: [], files: {} };
 * const agent = createDeepAgent({
 *   model: anthropic('claude-sonnet-4-20250514'),
 *   backend: new CompositeBackend(
 *     new StateBackend(state),
 *     { '/persistent/': new FilesystemBackend({ rootDir: './persistent' }) }
 *   ), // Route files by path prefix
 * });
 * ```
 *
 * @example With middleware for logging and caching
 * ```typescript
 * import { createDeepAgent } from 'ai-sdk-deep-agent';
 * import { anthropic } from '@ai-sdk/anthropic';
 *
 * const loggingMiddleware = {
 *   wrapGenerate: async ({ doGenerate, params }) => {
 *     console.log('Model called with:', params.prompt);
 *     const result = await doGenerate();
 *     console.log('Model returned:', result.text);
 *     return result;
 *   },
 * };
 *
 * const agent = createDeepAgent({
 *   model: anthropic('claude-sonnet-4-20250514'),
 *   middleware: [loggingMiddleware],
 * });
 * ```
 *
 * @example With middleware factory for context access
 * ```typescript
 * import { FilesystemBackend } from 'ai-sdk-deep-agent';
 *
 * function createContextMiddleware(backend: BackendProtocol) {
 *   return {
 *     wrapGenerate: async ({ doGenerate }) => {
 *       const state = await backend.read('state');
 *       const result = await doGenerate();
 *       await backend.write('state', { ...state, lastCall: result });
 *       return result;
 *     },
 *   };
 * }
 *
 * const backend = new FilesystemBackend({ rootDir: './workspace' });
 * const agent = createDeepAgent({
 *   model: anthropic('claude-sonnet-4-20250514'),
 *   backend,
 *   middleware: createContextMiddleware(backend),
 * });
 * ```
 *
 * @example With performance optimizations
 * ```typescript
 * const agent = createDeepAgent({
 *   model: anthropic('claude-sonnet-4-20250514'),
 *   enablePromptCaching: true,
 *   toolResultEvictionLimit: 20000,
 *   summarization: {
 *     enabled: true,
 *     tokenThreshold: 170000,
 *     keepMessages: 6,
 *   },
 * });
 * ```
 */
export function createDeepAgent(params: CreateDeepAgentParams): DeepAgent {
  return new DeepAgent(params);
}

// Re-export useful AI SDK v6 primitives
export { ToolLoopAgent, stepCountIs, hasToolCall } from "ai";
